# -*- coding: utf-8 -*-
"""NT@B F24

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BZG8uK6YUqL9aYi-Dk8pFQcsqfyhE_HJ

# How to Run This Notebook

1. Run every block that is labeled as [REQUIRED]

# [REQUIRED] Setup
"""

from google.colab import drive
drive.mount('/content/drive')

"""# [TEST] Initial Data Analysis"""

#!unzip /content/drive/"MyDrive"/"NTAB_MRI"/kaggle_BRATS_dataset.zip -d /content/drive/"MyDrive"/"NTAB_MRI"/"kaggle_BRATS_dataset"

# !ls /content/drive/"MyDrive"/"NTAB_MRI"/kaggle_BRATS_dataset/BraTS2020_training_data/content/data

# Count number of itmes in data folder (should be 57,198)
# !ls -1 /content/drive/"MyDrive"/"NTAB_MRI"/kaggle_BRATS_dataset/BraTS2020_training_data/content/data | wc -l

import matplotlib.pyplot as plt
import h5py

### UNDERSTANDING DATASET --> (Sample of input MRI image & corresponding segmentation map)

# Open the .h5 file
brats_data_folder = "/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS2020_training_data/content/data/"
file_name = "volume_10_slice_10.h5"

def examine_file(file_name: str):
  file_path = brats_data_folder + file_name
  print("######################")
  print("Examining", file_name)
  print("######################")
  with h5py.File(file_path, 'r') as f:
      # List all groups (like directories in a file system)
      print("Keys:", list(f.keys()))
      # Access first dataset in file
      dataset_name = list(f.keys())[1]
      data = f[dataset_name]
      print("Dataset_name:", dataset_name)
      print("Data shape:", data.shape)
      # print("Data contents:", data[:])  # Print contents

      #IMAGE PROCESSING

      # Access the 'image' dataset
      image_data = f['image'][:]
      print("type of image data", type(image_data))
      print("image data shape:", image_data.shape)
      # We think this is CNYK?

      # Normalize image data to range [0, 1] for better visualization
      normalized_image_data = (image_data - image_data.min()) / (image_data.max() - image_data.min())
      #print("Image Data:")
      # print("Normalized Image Data:")
      # print(normalized_image_data)
      # Display the image
      # print("Rendered normalized image:")
      # plt.imshow(normalized_image_data)
      # plt.axis('off')
      # plt.show()

      #MASK PROCESSING

      # Access the 'mask' dataset (segmentation map, ground truth labels)
      mask_data = f['mask'][:]
      print("type of mask data", type(mask_data))
      print("mask data shape:", mask_data.shape)

      # Normalize to range [0, 1]
      # normalized_mask_data = (mask_data - mask_data.min()) / (mask_data.max() - mask_data.min())
      #Print Mask Data
      # print("Mask Data:")
      # print(mask_data[0, 0, :])
      #print("Normalized Mask Data:")
      ##print(normalized_mask_data)

      # Display the image
      # print("Rendered normalized mask:")
      # plt.imshow(normalized_mask_data)
      # plt.axis('off')
      # plt.show()

#examine_file("volume_50_slice_25.h5") # Target is 0
examine_file("volume_53_slice_75.h5") # Target is 1

"""# [ARCHIVE] Model Architecture w/o attention gates

"""

import torch
# Verify GPU is working
if not torch.cuda.is_available():
  raise RuntimeError("Change runtime in CoLab to use T4 GPU")
else:
  print("T4 GPU is available")

import torch.nn as nn

"""
Potential experimentation:
  - Experiment with 64 base channels (done by paper)
  - Experiment with padding = 0 to reduce dimension like the UNET paper
  - More encoder/decoder layers: more intricate patterns but may also increase overfitting/computation time
"""

# Adding a border of 1 pixel of 0s around the input feature map --> input & ouput feature map same dimensions
PADDING = 1
class UNet3D(nn.Module):
    # in_channels = 1 bc grayscale, base_channels = 32 (# of feature maps in 1st convo layer of encoder block)
    def __init__(self, in_channels=1, out_channels=3, base_channels=32, print_dim=False):
        super(UNet3D, self).__init__()
        self.print_dim = print_dim

        # Encoder: Down-sampling
        # Each encoder block has 2 conv layers + Relu Activation
        # 3 levels of encoding/decoding + 32 base channels (paper has 4 and 64)

        self.encoder1 = self.conv_relu_2x(in_channels, base_channels)
        self.pool1 = nn.MaxPool3d(2)

        # base_channel increases to capture more complex features
        self.encoder2 = self.conv_relu_2x(base_channels, base_channels * 2)
        self.pool2 = nn.MaxPool3d(2)

        self.encoder3 = self.conv_relu_2x(base_channels * 2, base_channels * 4)
        self.pool3 = nn.MaxPool3d(2)

        # Bottleneck: last layer before decoder path
        self.bottleneck = self.conv_relu_2x(base_channels * 4, base_channels * 8)

        # Decoder: Up-sampling
        # Each encoder block has up-convo + concatenation w corresponding encoder output

        self.upconv3 = nn.ConvTranspose3d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)
        # self.upconv3 = self.linear_upsample_and_conv(base_channels * 8, base_channels * 4)
        self.decoder3 = self.conv_relu_2x(base_channels * 8, base_channels * 4)

        self.upconv2 = nn.ConvTranspose3d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)
        # self.upconv2 = self.linear_upsample_and_conv(base_channels * 4, base_channels * 2)
        self.decoder2 = self.conv_relu_2x(base_channels * 4, base_channels * 2)

        self.upconv1 = nn.ConvTranspose3d(base_channels * 2, base_channels, kernel_size=2, stride=2)
        # self.upconv1 = self.linear_upsample_and_conv(base_channels * 2, base_channels)
        self.decoder1 = self.conv_relu_2x(base_channels * 2, base_channels)

        # Final layer: produces output segmentation map
        self.final_conv = nn.Conv3d(base_channels, out_channels, kernel_size=1)

    # Applies 2 3x3 convo layer followed by Relu act (blue arrow in U-Net)
    def conv_relu_2x(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=PADDING),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=PADDING),
            nn.ReLU(inplace=True)
        )

    # Upsampling layers (increase 3 dim by factor of 2)
    # Might give us smoother / better results than ConvTranspose3d, but might run a little slower
    # Takes avg instead of filling in with 0s
    def linear_upsample_and_conv(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=False),  # Trilinear upsampling
            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=PADDING)      # Convolution for refinement
        )

    # How input data moves through network during forward pass
    # Func specifies how layers interact to produce output
    # takes in input tensor (n-dim array) which is 3D MRI scan
    def forward(self, x):
        # Want skip connections to concatenate along channel dimension
        # Note: input dimensions are (batch, 4dchannel, depth, width, length)
        # or                         (4dchannel, depth, width, length)
        if len(x.shape) == 4:
          CHANNEL_DIM = 0
        elif len(x.shape) == 5:
          CHANNEL_DIM = 1
        else:
          print("Input shape:", x.shape)
          raise TypeError("Incorrect input passed into model. Check dimensions")

        # Encoder path
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))

        # Bottleneck
        bottleneck = self.bottleneck(self.pool3(enc3))

        # Decoder path
        dec3 = self.upconv3(bottleneck)

        dec3 = torch.cat((dec3, enc3), dim=CHANNEL_DIM)  # Skip connection
        dec3 = self.decoder3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=CHANNEL_DIM)  # Skip connection
        dec2 = self.decoder2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=CHANNEL_DIM)  # Skip connection
        dec1 = self.decoder1(dec1)

        if self.print_dim:
          print("Encoder output 1 shape", enc1.shape)
          print("Encoder output 2 shape", enc2.shape)
          print("Encoder output 3 shape", enc3.shape)
          print("Decoder output 3 shape", dec3.shape)
          print("Decoder output 2 shape", dec2.shape)
          print("Decoder output 1 shape", dec1.shape)

        # Final output
        return self.final_conv(dec1)

"""# [REQUIRED] Model Architecture"""

import torch
import torch.nn as nn
import torch.nn.functional as F

PADDING = 1  # Keep the same padding as in the original implementation

class AttentionGate(nn.Module):
    """
    Attention Gate module for 3D U-Net.
    Implemented based on Attention U-Net paper: https://arxiv.org/abs/1804.03999
    Adapted for 3D volumes and flexible with 4D/5D inputs.
    """
    def __init__(self, F_g, F_l, F_int):
        """
        :param F_g: Number of feature maps in gating signal
        :param F_l: Number of feature maps in input feature map
        :param F_int: Number of intermediate feature maps
        """
        super(AttentionGate, self).__init__()

        # Use Conv3d with appropriate padding for maintaining dimensions
        self.W_g = nn.Conv3d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True)
        self.W_x = nn.Conv3d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True)

        # Don't use BatchNorm to avoid dimension issues
        self.psi = nn.Sequential(
            nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),
            nn.Sigmoid()
        )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        """
        :param g: Gating signal (from coarser layer)
        :param x: Skip connection input
        :return: Attention weighted output
        """
        # Add batch dimension if needed (4D â†’ 5D)
        if len(g.shape) == 4:
            g = g.unsqueeze(0)  # [C,D,H,W] â†’ [1,C,D,H,W]
        if len(x.shape) == 4:
            x = x.unsqueeze(0)  # [C,D,H,W] â†’ [1,C,D,H,W]

        g1 = self.W_g(g)
        x1 = self.W_x(x)

        # Align the spatial dimensions of g1 and x1
        if g1.shape[2:] != x1.shape[2:]:
            g1 = F.interpolate(g1, size=x1.shape[2:], mode='trilinear', align_corners=False)

        psi = self.relu(g1 + x1)
        psi = self.psi(psi)

        # Remove batch dimension if input was 4D
        if len(g.shape) == 5 and g.shape[0] == 1 and len(x.shape) == 5 and x.shape[0] == 1:
            psi = psi.squeeze(0)
            x = x.squeeze(0)

        return x * psi


class AttentionGatedUNet3D(nn.Module):
    """
    3D U-Net architecture with attention gates.
    Based on the original UNet3D implementation but with added attention gates.
    Handles both 4D and 5D input formats.
    """
    def __init__(self, in_channels=1, out_channels=4, base_channels=32, print_dim=False, use_attention=True):
        super(AttentionGatedUNet3D, self).__init__()
        self.print_dim = print_dim
        self.use_attention = use_attention

        # Encoder: Down-sampling
        self.encoder1 = self.conv_relu_2x(in_channels, base_channels)
        self.pool1 = nn.MaxPool3d(2)

        self.encoder2 = self.conv_relu_2x(base_channels, base_channels * 2)
        self.pool2 = nn.MaxPool3d(2)

        self.encoder3 = self.conv_relu_2x(base_channels * 2, base_channels * 4)
        self.pool3 = nn.MaxPool3d(2)

        # Bottleneck
        self.bottleneck = self.conv_relu_2x(base_channels * 4, base_channels * 8)

        # Attention Gates
        if use_attention:
            self.attention3 = AttentionGate(F_g=base_channels * 4, F_l=base_channels * 4, F_int=base_channels * 2)
            self.attention2 = AttentionGate(F_g=base_channels * 2, F_l=base_channels * 2, F_int=base_channels)
            self.attention1 = AttentionGate(F_g=base_channels, F_l=base_channels, F_int=base_channels // 2)

        # Decoder: Up-sampling
        self.upconv3 = nn.ConvTranspose3d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)
        self.decoder3 = self.conv_relu_2x(base_channels * 8, base_channels * 4)

        self.upconv2 = nn.ConvTranspose3d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)
        self.decoder2 = self.conv_relu_2x(base_channels * 4, base_channels * 2)

        self.upconv1 = nn.ConvTranspose3d(base_channels * 2, base_channels, kernel_size=2, stride=2)
        self.decoder1 = self.conv_relu_2x(base_channels * 2, base_channels)

        # Final layer
        self.final_conv = nn.Conv3d(base_channels, out_channels, kernel_size=1)

    def conv_relu_2x(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=PADDING),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=PADDING),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        """
        Forward pass for both 4D and 5D inputs.
        For 4D inputs: [C, D, H, W] format
        For 5D inputs: [B, C, D, H, W] format
        """
        # Determine input format
        is_4d_input = False
        if len(x.shape) == 4:  # [C, D, H, W]
            is_4d_input = True
            CHANNEL_DIM = 0
            x = x.unsqueeze(0)  # Add batch dimension [1, C, D, H, W]
        elif len(x.shape) == 5:  # [B, C, D, H, W]
            CHANNEL_DIM = 1
        else:
            raise ValueError(f"Expected 4D or 5D input but got {len(x.shape)}D input with shape {x.shape}")

        # Print input shape for debugging
        if self.print_dim:
            print(f"Input shape: {x.shape}")

        # Encoder path
        enc1 = self.encoder1(x)
        if self.print_dim: print(f"enc1 shape: {enc1.shape}")

        x_pool1 = self.pool1(enc1)
        enc2 = self.encoder2(x_pool1)
        if self.print_dim: print(f"enc2 shape: {enc2.shape}")

        x_pool2 = self.pool2(enc2)
        enc3 = self.encoder3(x_pool2)
        if self.print_dim: print(f"enc3 shape: {enc3.shape}")

        x_pool3 = self.pool3(enc3)
        bottleneck = self.bottleneck(x_pool3)
        if self.print_dim: print(f"bottleneck shape: {bottleneck.shape}")

        # Decoder path
        dec3 = self.upconv3(bottleneck)
        if self.print_dim: print(f"dec3 pre-concat shape: {dec3.shape}")

        # Apply attention and handle concatenation properly
        if self.use_attention:
            if is_4d_input:
                # Handle 4D case by removing batch dim first
                dec3_att = dec3.squeeze(0)
                enc3_att = enc3.squeeze(0)
                att3 = self.attention3(dec3_att, enc3_att)
                dec3_cat = torch.cat((dec3_att, att3), dim=CHANNEL_DIM).unsqueeze(0)
            else:
                att3 = self.attention3(dec3, enc3)
                dec3_cat = torch.cat((dec3, att3), dim=CHANNEL_DIM)
        else:
            # Regular skip connection
            dec3_cat = torch.cat((dec3, enc3), dim=CHANNEL_DIM)

        dec3 = self.decoder3(dec3_cat)
        if self.print_dim: print(f"dec3 shape: {dec3.shape}")

        dec2 = self.upconv2(dec3)
        if self.print_dim: print(f"dec2 pre-concat shape: {dec2.shape}")

        # Apply attention for second decoder level
        if self.use_attention:
            if is_4d_input:
                dec2_att = dec2.squeeze(0)
                enc2_att = enc2.squeeze(0)
                att2 = self.attention2(dec2_att, enc2_att)
                dec2_cat = torch.cat((dec2_att, att2), dim=CHANNEL_DIM).unsqueeze(0)
            else:
                att2 = self.attention2(dec2, enc2)
                dec2_cat = torch.cat((dec2, att2), dim=CHANNEL_DIM)
        else:
          # Regular skip connection
          dec2_cat = torch.cat((dec2, enc2), dim=CHANNEL_DIM)

        dec2 = self.decoder2(dec2_cat)
        if self.print_dim: print(f"dec2 shape: {dec2.shape}")

        dec1 = self.upconv1(dec2)
        if self.print_dim: print(f"dec1 pre-concat shape: {dec1.shape}")

        # Apply attention for first decoder level
        if self.use_attention:
            if is_4d_input:
                dec1_att = dec1.squeeze(0)
                enc1_att = enc1.squeeze(0)
                att1 = self.attention1(dec1_att, enc1_att)
                dec1_cat = torch.cat((dec1_att, att1), dim=CHANNEL_DIM).unsqueeze(0)
            else:
                att1 = self.attention1(dec1, enc1)
                dec1_cat = torch.cat((dec1, att1), dim=CHANNEL_DIM)
        else:
            # Regular skip connection
            dec1_cat = torch.cat((dec1, enc1), dim=CHANNEL_DIM)

        dec1 = self.decoder1(dec1_cat)
        if self.print_dim: print(f"dec1 shape: {dec1.shape}")

        # Final convolution
        output = self.final_conv(dec1)
        if self.print_dim: print(f"Output shape: {output.shape}")

        # Remove batch dimension if input was 4D
        if is_4d_input:
            output = output.squeeze(0)

        return output

    def center_crop(self, tensor, target_shape):
        _, _, d, h, w = tensor.shape
        td, th, tw = target_shape

        d1 = (d - td) // 2
        h1 = (h - th) // 2
        w1 = (w - tw) // 2

        return tensor[:, :, d1:d1+td, h1:h1+th, w1:w1+tw]



# Example usage
def get_model(in_channels=1, out_channels=4, base_channels=16, print_dim=False, use_attention=True):
    """
    Helper function to create an Attention Gated 3D U-Net.

    :param in_channels: Number of input channels
    :param out_channels: Number of output channels (classes)
    :param base_channels: Number of base feature maps
    :param print_dim: Whether to print dimensions during forward pass
    :param use_attention: Whether to use attention gates
    :return: Initialized model
    """
    return AttentionGatedUNet3D(
        in_channels=in_channels,
        out_channels=out_channels,
        base_channels=base_channels,
        print_dim=print_dim,
        use_attention=use_attention
    )

# Example of creating and using the model with your specific input shape
"""
# Based on your error message, your input has shape [1, 152, 240, 240]
# This needs to be properly shaped before passing to the model

# For a 4D input (without batch dimension)
inputs = torch.randn(1, 152, 240, 240)  # [C, D, H, W]

# Create model
model = get_model(
    in_channels=1,           # Number of channels in input
    out_channels=4,          # Number of output classes from error
    base_channels=32,
    print_dim=True,
    use_attention=True
)

# Forward pass
outputs = model(inputs)
print(f"Outputs shape: {outputs.shape}")
"""

"""# Patch Model Architecture"""

import torch
import torch.nn as nn
import torch.nn.functional as F

PADDING = 1  # Keep the same padding as in the original implementation

class AttentionGatePatch(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super(AttentionGatePatch, self).__init__()
        self.W_g = nn.Conv3d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True)
        self.W_x = nn.Conv3d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True)
        self.psi = nn.Sequential(
            nn.Conv3d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
        if g.dim() == 4:
            g = g.unsqueeze(0)
        if x.dim() == 4:
            x = x.unsqueeze(0)

        # print("[AttentionGatePatch] g shape:", g.shape)
        # print("[AttentionGatePatch] x shape:", x.shape)

        g1 = self.W_g(g)
        x1 = self.W_x(x)

        # print("[AttentionGatePatch] g1 shape:", g1.shape)
        # print("[AttentionGatePatch] x1 shape:", x1.shape)

        if g1.shape[2:] != x1.shape[2:]:
            g1 = F.interpolate(g1, size=x1.shape[2:], mode='trilinear', align_corners=False)

        psi = self.relu(g1 + x1)
        psi = self.psi(psi)

        # print("[AttentionGatePatch] psi shape:", psi.shape)

        return x * psi

class AttentionGatedUNet3DPatch(nn.Module):
    def __init__(self, in_channels=1, out_channels=4, base_channels=18, print_dim=False, use_attention=True):
        super(AttentionGatedUNet3DPatch, self).__init__()
        self.print_dim = print_dim
        self.use_attention = use_attention

        self.encoder1 = self.conv_relu_2x(in_channels, base_channels)
        self.pool1 = nn.MaxPool3d(2)
        self.encoder2 = self.conv_relu_2x(base_channels, base_channels * 2)
        self.pool2 = nn.MaxPool3d(2)

        self.bottleneck = self.conv_relu_2x(base_channels * 2, base_channels * 4)

        if use_attention:
            self.attention2 = AttentionGatePatch(F_g=base_channels * 2, F_l=base_channels * 2, F_int=base_channels)
            self.attention1 = AttentionGatePatch(F_g=base_channels, F_l=base_channels, F_int=base_channels // 2)

        self.upconv2 = nn.ConvTranspose3d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)
        self.decoder2 = self.conv_relu_2x(base_channels * 4, base_channels * 2)

        self.upconv1 = nn.ConvTranspose3d(base_channels * 2, base_channels, kernel_size=2, stride=2)
        self.decoder1 = self.conv_relu_2x(base_channels * 2, base_channels)

        self.final_conv = nn.Conv3d(base_channels, out_channels, kernel_size=1)

    def conv_relu_2x(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=PADDING),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=PADDING),
            nn.ReLU(inplace=True)
        )

    def center_crop(self, tensor, target_shape):
        # print("[center_crop] tensor shape:", tensor.shape)
        # print("[center_crop] target_shape:", target_shape)

        _, _, d, h, w = tensor.shape
        td, th, tw = target_shape
        d1 = (d - td) // 2
        h1 = (h - th) // 2
        w1 = (w - tw) // 2

        return tensor[:, :, d1:d1+td, h1:h1+th, w1:w1+tw]

    def forward(self, x):
        is_4d_input = False
        if x.dim() == 4:
            is_4d_input = True
            x = x.unsqueeze(0)
            CHANNEL_DIM = 1  # Corrected from 0 â†’ 1
        elif x.dim() == 5:
            CHANNEL_DIM = 1
        else:
            raise ValueError(f"Expected 4D or 5D input but got {x.dim()}D")

        enc1 = self.encoder1(x)
        x_pool1 = self.pool1(enc1)
        enc2 = self.encoder2(x_pool1)
        x_pool2 = self.pool2(enc2)
        bottleneck = self.bottleneck(x_pool2)

        dec2 = self.upconv2(bottleneck)
        if self.use_attention:
            att2 = self.attention2(dec2, enc2)
            att2 = self.center_crop(att2, dec2.shape[2:])
            # print("[Decoder2] dec2 shape:", dec2.shape)
            # print("[Decoder2] att2 shape:", att2.shape)
            dec2_cat = torch.cat((dec2, att2), dim=CHANNEL_DIM)
        else:
            enc2 = self.center_crop(enc2, dec2.shape[2:])
            dec2_cat = torch.cat((dec2, enc2), dim=CHANNEL_DIM)
        dec2 = self.decoder2(dec2_cat)

        dec1 = self.upconv1(dec2)
        if self.use_attention:
            att1 = self.attention1(dec1, enc1)
            att1 = self.center_crop(att1, dec1.shape[2:])
            # print("[Decoder1] dec1 shape:", dec1.shape)
            # print("[Decoder1] att1 shape:", att1.shape)
            dec1_cat = torch.cat((dec1, att1), dim=CHANNEL_DIM)
            #print("[Decoder1] dec1_cat shape:", dec1_cat.shape)
        else:
            enc1 = self.center_crop(enc1, dec1.shape[2:])
            dec1_cat = torch.cat((dec1, enc1), dim=CHANNEL_DIM)
            # print("[Decoder1] dec1_cat shape (no attention):", dec1_cat.shape)
        dec1 = self.decoder1(dec1_cat)
        # print("[Decoder1] dec1 final shape:", dec1.shape)

        output = self.final_conv(dec1)
        # print("[Output] final_conv output shape:", output.shape)

        if is_4d_input:
            output = output.squeeze(0)
        return output

"""# [TEST] Testing Model Architecture"""

# Example usage
model = UNet3D(in_channels=1, out_channels=2, print_dim=True)  # Assuming grayscale input and binary output
model.cuda()
# (We'll want to change this to 1 channel input and likely 4 output for segmentation)
"""
We have 170 slices per patient, but we will have to drop the top and bottom slides because
Skip connections dictate our depth must be divisible by 8. 168 is closest multiple of 8
"""
BATCH_SIZE = 1
FOURTH_CHANNEL_DIMENSION = 1
DEPTH = 168
WIDTH = 240
HEIGHT = 240
sample_patient_with_batch = torch.rand(BATCH_SIZE, FOURTH_CHANNEL_DIMENSION, DEPTH, WIDTH, HEIGHT).cuda()
output1 = model(sample_patient_with_batch).shape

sample_patient_without_batch = torch.rand(FOURTH_CHANNEL_DIMENSION, DEPTH, WIDTH, HEIGHT).cuda()
output2 = model(sample_patient_without_batch).shape
print("Output with batch shape:", output1)
print("Output without batch shape:", output2)

"""## [DEPRECATED TESTS]
Previously under Model Architecture
"""

# I wouldn't recommend uncommenting this because this dataset class is old
# %run /content/drive/MyDrive/NTAB_MRI/MRIDataset_Class.ipynb

# Define the dataset and dataloader
brats_data_folder = '/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS2020_training_data/content/data'
metadata_file = '/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS20_Training_Metadata.csv' # Path to your metadata file

dataset = MRIDataset(brats_data_folder, metadata_file)
#dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# Create a subset of the dataset with just one volume for testing
mri_dataset_subset = torch.utils.data.Subset(dataset, indices=[0])  # Use index 0 for the first volume
testdataloader = DataLoader(mri_dataset_subset, batch_size=1, shuffle=True)

# Sample code to pass data through the model
model = UNet3D(in_channels=1, out_channels=3, print_dim=True)
# for volume_data in dataloader:
#     output = model(volume_data)
#     print("Output shape:", output.shape)
# Assuming your DataLoader and model are already set up

# Fetch the first batch from the DataLoader
first_batch = next(iter(testdataloader))

# Unpack the images and masks
images, masks = first_batch  # Assuming the DataLoader returns a tuple of (images, masks)

# Check if images is a Tensor or a list
if isinstance(images, list):
    images = torch.tensor(images)  # Convert to tensor if it is a list

# Check the shape of the image (1 volume with all its slices)
print(f"Images shape: {images.shape}")  # Should be (batch_size, channels, depth, height, width) --> (1,1, 155, 240, 240)

#Make sure the model is in evaluation mode (to disable dropout, batch normalization, etc.)
model.eval()

# Pass the volume through the model (this will give you the model's prediction)
with torch.no_grad():  # No gradients needed for inference
    output = model(first_batch)

# Check the output shape
print(f"Output shape: {output.shape}")  # Should be (1, out_channels, 155, 240, 240)

# Optionally, visualize the output (for instance, the first slice in the predicted output)
import matplotlib.pyplot as plt

# QUESTION: if color should be 5, if grayscale should be 4
# Get the first slice of the output volume (assume output is in [batch, channels, depth, height, width])
predicted_slice = output[0, 0, 0, :, :]  # First slice of the first volume and first channel (if you're using grayscale)

# Visualize the predicted slice
plt.imshow(predicted_slice.squeeze(), cmap='gray')
plt.title("Predicted slice (first slice of first volume)")
plt.show()

"""**Some benchmarks** (while printing dimension) <p>
Time to run on 168 x 240 x 240 with custom upsample function: -- unknown, says it used all the RAM lmao
<p>
Time to run on 168 x 240 x 240 with chatGPT upsample inverse convolution function: 1m 18s
"""



"""# [CONVERSION] Converting H5 Files into Pytorch Tensors
This does not have to be run every time for the model to run properly, but it isn't a test block so I didn't mark it as one lol

**Do not run without mounting the Google Drive!**
"""

import os
import torch
import h5py
import numpy as np

NUM_SLICES = 155
LAST_PATIENT_ID = 369
BRATS_DATA_FOLDER = "/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS2020_training_data/content/data"
def h5_to_tensor(volume_id, grayscale):

    # Define the path to save the tensor in Google Drive
    save_dir = "/content/drive/MyDrive/NTAB_MRI/processed_h5"

    # Init empty tensor to fill
    if grayscale:
        slices = np.empty((NUM_SLICES, 240, 240))
        save_path = os.path.join(save_dir, f"grayscale_volume_{volume_id}.pt")
    else:
        slices = np.empty((NUM_SLICES, 240, 240, 4))
        save_path = os.path.join(save_dir, f"volume_{volume_id}.pt")


    # Load each slice and stack them to form a 3D volume
    for slice_idx in range(NUM_SLICES):
        slice_file = f"{BRATS_DATA_FOLDER}/volume_{volume_id}_slice_{slice_idx}.h5"
        with h5py.File(slice_file, 'r') as f:
            image_data = f['image'][:]  # Adjust 'image' if your dataset key is different

            if grayscale:
                processed_image = np.mean(image_data, axis=-1)
            else:
                if image_data.max() == image_data.min():
                    processed_image = [image_data.max()] * 4
                else:
                    processed_image = (image_data - image_data.min()) / (image_data.max() - image_data.min())

            slices[slice_idx] = processed_image

    # Convert slices to a PyTorch tensor and add channel dim
    volume_data = torch.tensor(slices, dtype=torch.float32).unsqueeze(0)

    # Save the tensor to Google Drive
    # torch.save(volume_data, save_path)
    # print(f"Saved volume tensor to {save_path}")

    ### Repeat for mask
    slices = np.empty((NUM_SLICES, 240, 240, 3))

    # Load each slice and stack them to form a 3D volume
    for slice_idx in range(NUM_SLICES):
        slice_file = f"{BRATS_DATA_FOLDER}/volume_{volume_id}_slice_{slice_idx}.h5"
        with h5py.File(slice_file, 'r') as f:
            # Retrieve the mask data instead of the image
            mask_data = f['mask'][:]  # Adjust 'mask' if your dataset key is different
            slices[slice_idx] = mask_data

    mask_data = torch.tensor(slices, dtype=torch.float32)
    save_path = os.path.join(save_dir, f"mask_volume_{volume_id}.pt")
    torch.save(mask_data, save_path)
    print(f"Saved mask tensor to {save_path}")


for patient_id in range(1, 51):
  h5_to_tensor(patient_id, grayscale=True)

"""# [CONVERSION] Adding Control Class to Mask

Initial Investigations on adding a fourth control channel.
"""

import os
import torch
import matplotlib.pyplot as plt

MASK_TENSORS_DIR = "/content/drive/MyDrive/NTAB_MRI/processed_h5"

tensor_path = os.path.join(MASK_TENSORS_DIR, f"mask_volume_{1}.pt")

mask_data = torch.load(tensor_path, weights_only=True)
idx = 0
# idx 68 has all 3 colors
slice_68 = mask_data[68]
coords_with_values = []
for x in range(240):
  for y in range(240):
    if any(slice_68[x][y]):
      coords_with_values.append((x,y))

case0 = (slice_68.sum(dim=-1, keepdim=True) == 0).int()  # Shape WxLxDx1


# Concatenate case0 as the first channel
slice_68_with_case0 = torch.cat((case0, slice_68), dim=-1)  # Shape WxLxDx4

print("original mask; shape",slice_68.shape)
plt.imshow(slice_68)
plt.axis('off')
plt.show()

print("Mask with case zero added; shape", slice_68_with_case0.shape)
plt.imshow(slice_68_with_case0)
plt.axis('off')
plt.show()

reconstruction = slice_68_with_case0[:, :, 1:]
print("Original mask reconstructed from case 0; shape", reconstruction.shape)
plt.imshow(reconstruction)
plt.axis('off')
plt.show()

assert torch.equal(reconstruction, slice_68), "Reconstructed mask is not the same as the original"
channel_wise_sum = torch.sum(slice_68_with_case0, axis=-1)
assert torch.equal(channel_wise_sum, torch.ones(240, 240)), "New augmented mask is not an array of mutually exclusive binaries"

"""Script that actually adds the fourth channel to all the existing saved tensors"""

import os
import torch

MASK_TENSORS_DIR = "/content/drive/MyDrive/NTAB_MRI/processed_h5"

for patient_id in range(226, 297):
  print("Converting", patient_id)
  tensor_path = os.path.join(MASK_TENSORS_DIR, f"mask_volume_{patient_id}.pt")
  mask_data = torch.load(tensor_path, weights_only=True)
  pixel_is_healthy = (mask_data.sum(dim=-1, keepdim=True) == 0).int()  # Shape WxLxDx1
  new_mask_with_healthy = torch.cat((pixel_is_healthy, mask_data), dim=-1)  # Shape WxLxDx4
  save_path = os.path.join(MASK_TENSORS_DIR, f"mask_volume_with_healthy_{patient_id}.pt")
  torch.save(new_mask_with_healthy, save_path)

"""# [REQUIRED] Data Pipeline

"""

import os
import h5py
import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
PROCESSED_H5_TENSORS_DIR = "/content/drive/MyDrive/NTAB_MRI/processed_h5/"
class MRIDataset(Dataset):
    def __init__(self, brats_data_folder, metadata_file, start_slice=0, end_slice=155, transform=None, grayscale=False):
        """
        Args:
            brats_data_folder (str): Path to the folder containing the .h5 MRI data
            metadata_file (str): Path to the CSV metadata file
            min_slices (int): Minimum number of slices to sample
            max_slices (int): Maximum number of slices to sample
            transform (callable, optional): Optional transform to be applied to the images and masks
        """
        self.brats_data_folder = brats_data_folder
        self.metadata_file = metadata_file
        self.start_slice = start_slice
        self.end_slice = end_slice
        self.transform = transform
        self.grayscale = grayscale

        # Load metadata
        self.metadata = pd.read_csv(metadata_file)
        self.metadata.sort_values("volume", ascending=True, inplace=True)

        # Create a list of file names (assuming file names are structured properly in the folder)
        self.file_names = self.metadata['volume'].unique()

        """

        I disagree with maintaining a "healthy_vols" data structure because
        we know our dataset is full of cancerous data. This is our ground truth
        data that has been labeled by doctors, so there shouldn't be any healthy
        volumes I don't think - Evan
        """

         # Array of healthy volumes in the dataset
        self.healthy_vols = []

        # volume is "healthy" if it has mostly healthy slices (not necessarily ALL)
        healthy_percentage_threshold = 0.95
        for vol in self.file_names:
            vol_df = self.metadata[self.metadata['volume'] == vol]
            num_slices = len(vol_df)
            num_unhealthy_slices = np.sum(vol_df["target"])
            if num_unhealthy_slices / num_slices < (1 - healthy_percentage_threshold):
                self.healthy_vols.append(vol)

    def __len__(self):
        # Return the number of volumes in the dataset
        return len(self.file_names)

    # returns image data for a given volume index
    def __getitem__(self, idx):
        volume_id = self.file_names[idx]

        # Construct the file path for the tensor
        if self.grayscale:
          tensor_path = os.path.join(PROCESSED_H5_TENSORS_DIR, f"grayscale_volume_{volume_id}.pt")
        else:
          tensor_path = os.path.join(PROCESSED_H5_TENSORS_DIR, f"grayscale_volume_{volume_id}.pt")

        if os.path.exists(tensor_path):
            # Load the tensor using torch.load
            volume_data = torch.load(tensor_path, weights_only=True)
            # print(f"Loaded tensor for volume {volume_id} from {tensor_path}")
        else:
            raise FileNotFoundError(f"Tensor file for volume {volume_id} not found at {tensor_path}")

        # slice data
        volume_data = volume_data[:, self.start_slice:self.end_slice, :, :]


        # Apply transformations if any
        if self.transform:
            volume_data = self.transform(volume_data)
        return (volume_data, self.getMask(idx))

#train_dataset = some instance of MRIDatase
#train_dataloader = Dataloader(train_dataset, batch_size=..., shuffle=True)

    def getMask(self, idx):
      volume_id = self.file_names[idx]

      # Construct the file path for the tensor
      tensor_path = os.path.join(PROCESSED_H5_TENSORS_DIR, f"mask_volume_with_healthy_{volume_id}.pt")

      if os.path.exists(tensor_path):
          # Load the tensor using torch.load
          mask_data = torch.load(tensor_path, weights_only=True)
          # print(f"Loaded tensor for mask {volume_id} from {tensor_path}")
      else:
          raise FileNotFoundError(f"Mask file for volume {volume_id} not found at {tensor_path}")
      # slice data
      mask_data = mask_data[self.start_slice:self.end_slice, :, :, :]
      return mask_data


# Example usage
brats_data_folder = "/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS2020_training_data/content/data"
# I lowkey think this metadata file is useless (evan)
metadata_file = "/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS20_Training_Metadata.csv"

class MetadataGuidedPatchDataset(torch.utils.data.Dataset):
    def __init__(self, metadata_csv, patch_size=(64, 128, 128), grayscale=True):
        self.metadata = pd.read_csv(metadata_csv)
        self.patch_size = patch_size
        self.grayscale = grayscale

        # Filter to only tumor slices
        self.metadata = self.metadata[
            (self.metadata['label0_pxl_cnt'] > 0) |
            (self.metadata['label1_pxl_cnt'] > 0) |
            (self.metadata['label2_pxl_cnt'] > 0)
        ].reset_index(drop=True)

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        row = self.metadata.iloc[idx]
        vol_id = row['volume']
        slice_id = row['slice']

        # Load tensors
        input_path = os.path.join(PROCESSED_H5_TENSORS_DIR, f"grayscale_volume_{vol_id}.pt")
        mask_path  = os.path.join(PROCESSED_H5_TENSORS_DIR, f"mask_volume_with_healthy_{vol_id}.pt")

        volume = torch.load(input_path, weights_only=True)
        mask   = torch.load(mask_path, weights_only=True)

        # Extract patch around this slice
        z = int(slice_id)
        d_half = self.patch_size[0] // 2
        z_start = max(0, z - d_half)
        z_end = z_start + self.patch_size[0]

        h, w = volume.shape[2], volume.shape[3]
        y_start = (h - self.patch_size[1]) // 2
        x_start = (w - self.patch_size[2]) // 2

        volume_patch = volume[:, z_start:z_end, y_start:y_start + self.patch_size[1], x_start:x_start + self.patch_size[2]]
        mask_patch   = mask[z_start:z_end, y_start:y_start + self.patch_size[1], x_start:x_start + self.patch_size[2], :]


        # If the patch is not the expected shape, skip or fix
        expected_shape = (1, 64, 128, 128)
        if volume_patch.shape != expected_shape:
          print(f"[WARNING] Skipping idx {idx} due to shape mismatch: {volume_patch.shape}")
          return self.__getitem__((idx + 1) % len(self))  # or just return a blank patch

        return volume_patch, mask_patch

"""# [TEST] Testing Data Pipeline"""

USE_GRAYSCALE = True

dataset = MRIDataset(brats_data_folder, metadata_file, start_slice=0, end_slice=155, grayscale=USE_GRAYSCALE)

import matplotlib.pyplot as plt
def show_all_slices(patient):
  for image_slice in patient[0][0]:
    plt.imshow(image_slice)
    plt.axis('off')
    plt.show()

def show_all_mask_slices(patient, reconstruct=True):
  count = 0
  for image_slice in patient[1]:
    if reconstruct:
      # recreate RGB from RGBA format
      image_slice =  image_slice[:, :, 1:]
    plt.imshow(image_slice)
    plt.axis('off')
    plt.show()

# Testing dataloader
# It seems like accessing a patient runs much slower the first time it's run
# Maybe there's some caching that goes on when it opens files

patient_0 = dataset[0]
patient_368 = dataset[368]
print("dataset size (should be 369):", len(dataset))
show_all_slices(patient_0)
show_all_mask_slices(patient_0)
# patient_369 = dataset[369] # this should error

"""# [VISUALIZATION] Data Animation"""

import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.animation import PillowWriter
from IPython.display import HTML
import numpy as np

"""Animation for slices"""

def animate_slices(patient, interval=100, save_as_gif=False, gif_filename="slices.gif"):
    """
    Animate all slices of the patient's data smoothly and save as a GIF if required.

    Args:
        patient: The patient data containing slices to display (patient[0][0]).
        interval: Time between frames in milliseconds.
        save_as_gif: Whether to save the animation as a GIF.
        gif_filename: Filename for the saved GIF.
    """
    slices = patient[0][0]  # Extract the slices
    num_slices = slices.shape[0]  # Total number of slices

    slices = slices.cpu().numpy()

    slices = slices.astype(np.float32)

    # Create a figure and axis
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.axis('off')  # Turn off axes

    vmin = np.percentile(slices, 5)  # 5th percentile
    vmax = np.percentile(slices, 95)  # 95th percentile

    img = ax.imshow(slices[0], vmin=vmin, vmax=vmax)  # Adjust vmin/vmax if needed

    # Function to update the image for each frame
    def update(frame):
        img.set_data(slices[frame])  # Update image data
        return [img]

    # Create the animation
    ani = animation.FuncAnimation(
        fig, update, frames=num_slices, interval=interval, blit=True
    )

    # Display animation in the notebook
    display(HTML(ani.to_jshtml()))

    # Save as a video if requested
    if save_as_gif:
        writer = PillowWriter(fps=1000 // interval)  # FPS is derived from interval
        ani.save(gif_filename, writer=writer)
        print(f"GIF saved as {gif_filename}")

#Run slices animation
patient_0 = dataset[0]
patient_311 = dataset[311]
animate_slices(patient_311, interval=100, save_as_gif=True, gif_filename="patient_311.gif")

"""Animation for Mask"""

def animate_mask(patient, interval=100, save_as_gif=False, gif_filename="mask.gif"):
    """
    Animate all slices of the patient's data smoothly and save as a GIF if required.

    Args:
        patient: The patient data containing slices to display (patient[0][0]).
        interval: Time between frames in milliseconds.
        save_as_gif: Whether to save the animation as a GIF.
        gif_filename: Filename for the saved GIF.
    """
    slices = patient[1]  # Extract the slices
    num_slices = slices.shape[0]  # Total number of slices

    slices = slices.cpu().numpy()

    slices = slices.astype(np.float32)

    # Create a figure and axis
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.axis('off')  # Turn off axes

    vmin = np.percentile(slices, 5)  # 5th percentile
    vmax = np.percentile(slices, 95)  # 95th percentile

    img = ax.imshow(slices[0], vmin=vmin, vmax=vmax)  # Adjust vmin/vmax if needed

    # Function to update the image for each frame
    def update(frame):
        img.set_data(slices[frame])  # Update image data
        return [img]

    # Create the animation
    ani = animation.FuncAnimation(
        fig, update, frames=num_slices, interval=interval, blit=True
    )

    # Display animation in the notebook
    display(HTML(ani.to_jshtml()))

    # Save as a video if requested
    if save_as_gif:
        writer = PillowWriter(fps=1000 // interval)  # FPS is derived from interval
        ani.save(gif_filename, writer=writer)
        print(f"GIF saved as {gif_filename}")

#Run mask animation
patient_0 = dataset[0]
patient_311 = dataset[311]
animate_mask(patient_311, interval=100, save_as_gif=True, gif_filename="patient_311_mask.gif")

"""# [TRAINING] Loss Functions"""

import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import torch.nn as nn

# Dice Loss !  (loss func that computes region to region comparisons of prediction and ground truth segmentation map)
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth  # Small value to avoid division by zero

    # Emmas func
    # We should do something similar to ensure that this function is flexible
    # for 5-d and 4-d inputs
    def forward(self, logits, targets):
        # Note: logits are stored as (channels, depth, width, length)
        # and  targets are stored as (depth, width, length, channels)
        # Permute so these match (channels, depth, width, length)
        targets = targets.permute(3, 0, 1, 2)
        # print("targets shape", targets.shape)

        # print("=== Testing logits before softmax ===")
        # print("Channel 1:", logits[0][0][0][0])
        # print("Channel 2:", logits[1][0][0][0])
        # print("Channel 3:", logits[2][0][0][0])

        # Convert model's raw output to probabilities using softmax
        probs = torch.softmax(logits, dim=0)  # Apply softmax to get class probabilities
        # print("probs shape", probs.shape)
        # print("=== Testing logits after softmax ===")
        # print("Channel 1 prob:", probs[0][0][0][0])
        # print("Channel 2 prob:", probs[1][0][0][0])
        # print("Channel 3 prob:", probs[2][0][0][0])

        # # Add a background channel to probs based on the threshold
        # background_channel = torch.all(probs < self.threshold, dim=0).unsqueeze(0).float()  # Check if all channels are below the threshold
        # probs = torch.cat([probs, background_channel], dim=0)  # Add the background channel

        # Convert the target to one-hot encoding (binary format per class)
        # num_classes = logits.shape[0]
        # # targets = torch.argmax(targets, dim=0)  # Get the class index for each voxel (in targets)

        # # Create targets_onehot with correct shape, including background
        # targets_onehot = torch.eye(num_classes, device=targets.device)[targets]
        # targets_onehot = targets_onehot.permute(3, 0, 1, 2).float()  # Change the permutation

        # # Add background channel to targets_onehot if not already present
        # if targets_onehot.shape[0] != num_classes:
        #     background_targets = (targets == 0).unsqueeze(0).float()  # Assume background class is 0 in targets
        #     targets_onehot = torch.cat([targets_onehot, background_targets], dim=0)

        # # Move targets_onehot to the appropriate device (CUDA if available)
        # if logits.is_cuda:
        #     targets_onehot = targets_onehot.cuda()


        # print("targets shape", targets.shape)
        # print("probs shape", probs.shape)

        # Calculate intersection and union for Dice score - use formula
        intersection = torch.sum(probs * targets, dim=(1, 2, 3))  # Sum the intersection across spatial dims
        union = torch.sum(probs + targets, dim=(1, 2, 3))  # Sum the union across spatial dims
        # print("intersection shape", intersection.shape)
        # print("union shape", union.shape)
        # Compute dice score (overlap between predicted and true areas)
        dice_score = (2.0 * intersection + self.smooth) / (union + self.smooth)

        # Return the Dice loss value yay
        dice_loss = 1 - dice_score.mean()
        return dice_loss

import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import torch.nn as nn

class WeightedDiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(WeightedDiceLoss, self).__init__()
        self.smooth = smooth  # Small value to avoid division by zero

    # Emmas func
    # We should do something similar to ensure that this function is flexible
    # for 5-d and 4-d inputs
    def forward(self, logits, targets, weights_tensor):
        # Note: logits are stored as (channels, depth, width, length)
        # and  targets are stored as (depth, width, length, channels)
        # Permute so these match (channels, depth, width, length)
        targets = targets.permute(3, 0, 1, 2)
        # print("targets shape", targets.shape)

        # print("=== Testing logits before softmax ===")
        # print("Channel 1:", logits[0][0][0][0])
        # print("Channel 2:", logits[1][0][0][0])
        # print("Channel 3:", logits[2][0][0][0])

        # Convert model's raw output to probabilities using softmax
        probs = torch.softmax(logits, dim=0)  # Apply softmax to get class probabilities
        # print("probs shape", probs.shape)
        # print("=== Testing logits after softmax ===")
        # print("Channel 1 prob:", probs[0][0][0][0])
        # print("Channel 2 prob:", probs[1][0][0][0])
        # print("Channel 3 prob:", probs[2][0][0][0])

        # # Add a background channel to probs based on the threshold
        # background_channel = torch.all(probs < self.threshold, dim=0).unsqueeze(0).float()  # Check if all channels are below the threshold
        # probs = torch.cat([probs, background_channel], dim=0)  # Add the background channel

        # Convert the target to one-hot encoding (binary format per class)
        # num_classes = logits.shape[0]
        # # targets = torch.argmax(targets, dim=0)  # Get the class index for each voxel (in targets)

        # # Create targets_onehot with correct shape, including background
        # targets_onehot = torch.eye(num_classes, device=targets.device)[targets]
        # targets_onehot = targets_onehot.permute(3, 0, 1, 2).float()  # Change the permutation

        # # Add background channel to targets_onehot if not already present
        # if targets_onehot.shape[0] != num_classes:
        #     background_targets = (targets == 0).unsqueeze(0).float()  # Assume background class is 0 in targets
        #     targets_onehot = torch.cat([targets_onehot, background_targets], dim=0)

        # # Move targets_onehot to the appropriate device (CUDA if available)
        # if logits.is_cuda:
        #     targets_onehot = targets_onehot.cuda()


        # print("targets shape", targets.shape)
        # print("probs shape", probs.shape)

        # Calculate intersection and union for Dice score - use formula
        intersection = torch.sum(probs * targets, dim=(1, 2, 3), dtype=torch.double)  # Sum the intersection across spatial dims
        # print('intersection shape:', intersection.shape)
        # print(type(intersection[0]))

        #intersection = torch.dot(intersection, weights_tensor)

        union = torch.sum(probs + targets, dim=(1, 2, 3), dtype=torch.double)  # Sum the union across spatial dims
        # print("intersection shape", intersection.shape)
        # print("union shape", union.shape)
        # Compute dice score (overlap between predicted and true areas)
        dice_score = (2.0 * intersection + self.smooth) / (union + self.smooth)

        # Return the Dice loss value yay
        dice_loss = 1 - dice_score.mean()
        return dice_loss

class FTLLoss(nn.Module):
    def __init__(self, alpha=0.7, beta=0.3, gamma=4/3, smooth=1e-6):
        super(FTLLoss, self).__init__()
        self.alpha = alpha  # Weight for false negatives
        self.beta = beta    # Weight for false positives
        self.gamma = gamma  # Focusing parameter
        self.smooth = smooth  # Small value to avoid division by zero

    def forward(self, logits, targets):

        # Ensure target dimensions match the prediction
        # Permute targets to (channels, depth, width, height)
        targets = targets.permute(3, 0, 1, 2)

        # Convert logits to probabilities using softmax
        probs = torch.softmax(logits.clamp(-10, 10), dim=0)  # Class probabilities

        # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)
        true_pos = torch.sum(probs * targets, dim=(1, 2, 3))
        false_pos = torch.sum(probs * (1 - targets), dim=(1, 2, 3))
        false_neg = torch.sum((1 - probs) * targets, dim=(1, 2, 3))

        # Compute the Tversky index
        tversky_index = (true_pos + self.smooth) / (true_pos + self.alpha * false_neg + self.beta * false_pos + self.smooth)

        # Apply the focal term to emphasize hard examples
        focal_tversky_loss = torch.mean((1 - tversky_index) ** self.gamma)

        return focal_tversky_loss

"""# [TRAINING] Training Loop"""

# get label weights for each vol
import pandas as pd
metadata_file = "/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS20_Training_Metadata.csv"
metadata = pd.read_csv(metadata_file)
metadata = metadata.sort_values(by=['volume', 'slice'])
metadata = metadata[(metadata['slice'] > 1) & (metadata['slice'] < 154)]
#metadata.head()

total_vol_pixels = 152 * 240 * 240

summed = metadata.groupby('volume').sum()
summed.head()
summed['label0_proportion'] = summed['label0_pxl_cnt'] / total_vol_pixels
summed['label1_proportion'] = summed['label1_pxl_cnt'] / total_vol_pixels
summed['label2_proportion'] = summed['label2_pxl_cnt'] / total_vol_pixels
summed['background_proportion'] = (total_vol_pixels - (summed['label0_pxl_cnt'] + summed['label1_pxl_cnt'] + summed['label2_pxl_cnt'])) / total_vol_pixels
summed['label0_weight'] = 1 / summed['label0_proportion']
summed['label1_weight'] = 1 / summed['label1_proportion']
summed['label2_weight'] = 1 / summed['label2_proportion']
summed['background_weight'] = 1 / summed['background_proportion']
summed.head()

# Initialize model, loss function, & optimizer
# model = UNet3D(in_channels=1, out_channels=4, base_channels=18)
# model_path = "/content/drive/MyDrive/NTAB_MRI/best_model_old_lr_from_niki_41_epoch_1_loss_0.8136511549756333.pth"
# model.load_state_dict(torch.load(model_path))
# Example of how to create and use the model:
#
# # Create model
# model = get_model(in_channels=1, out_channels=4, base_channels=32, print_dim=True)
#
# # Sample input tensor (batch_size, channels, depth, height, width)
# x = torch.randn(1, 1, 64, 64, 64)
#
# # Forward pass
# output = model(x)
#
# print(f"Input shape: {x.shape}")
# print(f"Output shape: {output.shape}")

# model.cuda() # Put model on the GPU

model = AttentionGatedUNet3DPatch(in_channels=1, out_channels=4, base_channels=18)
model.cuda()


dice_loss = DiceLoss()
ftl_loss = FTLLoss()
criterion = FTLLoss() # Changed from WeightedDiceLoss
# Adam optimizer (best for good momentum & adaptive learning rate)
learning_rate = 1e-3
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Early stopping variables
patience = 5  # Num of epochs to wait for validation loss improvement
best_val_loss = float('inf')  # Initialize best validation loss to a very high value
epochs_without_improvement = 0  # Counter to track how many epochs since last improvement

# Create DataLoaders
# batch_size = 2  # Set your desired batch size, currently UNUSED
brats_data_folder = "/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS2020_training_data/content/data"
metadata_file = "/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS20_Training_Metadata.csv"
USE_GRAYSCALE = True

#dataset = MRIDataset(brats_data_folder, metadata_file, start_slice=2, end_slice=154, grayscale=USE_GRAYSCALE)
dataset = MetadataGuidedPatchDataset(
    metadata_csv="/content/drive/MyDrive/NTAB_MRI/kaggle_BRATS_dataset/BraTS20_Training_Metadata.csv",
    patch_size=(64, 128, 128),
    grayscale=True
)


# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Split into train and validation sets
train_size = int(0.8 * len(dataset))  # 80% for training
val_size = len(dataset) - train_size  # 20% for validation


# Training Loop (Training will stop if validation loss doesn't improve for 5 epochs)
epoch = 0

while epochs_without_improvement < patience:  # Loop until no improvement for 'patience' epochs
    epoch += 1
    model.train()  # Set model to training mode

    # Loop through batches in the training dataset
    # inputs: 3D MRI scans, targets: 3D segmented labels
    count = 0
    for batch_idx in range(0, train_size):
        count += 1
        # calculate weights for each label by pixel counting
        weights_array = []
        weights_array.append(summed['label0_weight'][batch_idx + 1])
        weights_array.append(summed['label1_weight'][batch_idx + 1])
        weights_array.append(summed['label2_weight'][batch_idx + 1])
        weights_array.append(summed['background_weight'][batch_idx + 1])

        weights_tensor = torch.tensor(weights_array, dtype=torch.double)
        print('weights:', weights_array)

        inputs, targets = dataset[batch_idx]
        print("inputs shape", inputs.shape)
        inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU if available (CHANGE?)

        # FORWARD PASS
        optimizer.zero_grad()  # Clear old gradients before the new backward pass
        outputs = model(inputs)  # Pass inputs through the model to get predictions

        # LOSS FUNC CALC
        #ipdb.set_trace()
        loss = ftl_loss(outputs, targets)  # Calculate the Dice loss between predictions and true labels
        our_dice_loss = dice_loss(outputs, targets)

        # BACKWARD PASS
        loss.backward()

        # PARAMETER UPDATING
        optimizer.step()

        print(f"Epoch [{epoch}], Batch [{batch_idx}], Criterion Loss: {loss.item():.4f}, Dice Loss = {our_dice_loss.item():.4f}")

        if count >= 150:
          torch.save(model.state_dict(), f"/content/drive/MyDrive/NTAB_MRI/intermediate_steps/" +
                    f"FTL_Loss_alpha{criterion.alpha}_beta{criterion.beta}_gamma{criterion.gamma}_lr{learning_rate}_E{epoch}_batch_{batch_idx}_" +
                    f"loss_{loss.item():.4f}.pth")
          count = 0

    # VALIDATION STEP
    model.eval()  # Set the model to evaluation mode
    val_loss = 0  # Initialize validation loss accumulator
    print("Starting Validation")
    with torch.no_grad():  # Disable gradient calculation to save memory during validation
        for batch_idx in range(train_size, len(dataset)):
            val_inputs, val_targets = dataset[batch_idx]
            if torch.cuda.is_available():
                val_inputs, val_targets = val_inputs.cuda(), val_targets.cuda()  # Move to GPU if available

            val_outputs = model(val_inputs)  # Get predictions for validation data
            this_loss = ftl_loss(val_outputs, val_targets).item()
            print("val batch", batch_idx, "loss:", this_loss)
            val_loss += this_loss  # Add the loss for this batch

    val_loss /= val_size  # Average validation loss over all batches
    print(f"Epoch [{epoch}], Validation Loss: {val_loss:.4f}")

    # EARLY STOPPING CHECK
    if val_loss < best_val_loss:  # Check if the validation loss improved
        best_val_loss = val_loss  # Update the best validation loss
        epochs_without_improvement = 0  # Reset the counter since we had improvement
        # Save the best model :)
        torch.save(model.state_dict(),
                   f"/content/drive/MyDrive/NTAB_MRI/" +
                   f"FTL_Loss_alpha{criterion.alpha}_beta{criterion.beta}_gamma{criterion.gamma}" +
                   f"lr{learning_rate}_E{epoch}_loss_{best_val_loss}.pth")
    else:
        epochs_without_improvement += 1  # Increment counter since there was no improvement

    # If no improvement for 'patience' epochs, training will stop
    if epochs_without_improvement >= patience:
        print("Early stopping triggered. Training has stopped.")
        break  # Exit the training loop

print("Training complete.")

"""# [VISUALIZATION] Compare Input, Mask, Output

This block of code visualizes single samples that have been cherry-picked to have a low loss (of around 0.5 from training set, and around 0.641 from validation).
"""

import matplotlib.pyplot as plt
import numpy as np
import torch

import os

# Create a directory to save the figures
save_dir = "mri_segmentation_pdfs"
os.makedirs(save_dir, exist_ok=True)

# Load data and model as before
dataset = MRIDataset(brats_data_folder, metadata_file, start_slice=2, end_slice=154, grayscale=True)

# model = AttentionGatedUNet3DPatch(in_channels=1, out_channels=4, base_channels=18)
# model_path = '/content/drive/MyDrive/NTAB_MRI/intermediate_steps/FTL_Loss_alpha0.7_beta0.3_gamma1.3333333333333333_lr0.001_E1_batch_299_loss_0.7550.pth'

model = UNet3D(in_channels=1, out_channels=3, base_channels=32)
model_path = "/content/drive/MyDrive/NTAB_MRI/FR_best_model_old_lr_from_niki_41_epoch_1_loss_0.8136511549756333.pth"

model.load_state_dict(torch.load(model_path))
model.cuda()
model.eval()

# Load sample
single_sample_inputs, single_sample_targets = dataset[311]
single_sample_inputs = single_sample_inputs.cuda().unsqueeze(0)  # Add batch dim

# Forward pass
with torch.no_grad():
    single_sample_outputs = model(single_sample_inputs)

# Remove batch dimension and softmax
single_sample_outputs = torch.softmax(single_sample_outputs.squeeze(0), dim=0)  # Shape: [C, D, H, W]
single_sample_preds = torch.argmax(single_sample_outputs, dim=0)                # Shape: [D, H, W]
single_sample_targets = torch.argmax(single_sample_targets, dim=-1)             # Shape: [D, H, W]
single_sample_inputs = single_sample_inputs.squeeze(0).cpu().numpy()            # Shape: [1, D, H, W]

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import torch

# Custom colormap: class 0 = red, class 1 = green, class 2 = blue
target_cmap = ListedColormap(['blue', 'red', 'green'])
preds_cmap = ListedColormap(['red', 'green', 'blue'])

# Visualize selected slices
for image_index in [75]:
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    axs[0].imshow(single_sample_inputs[0][image_index], cmap='gray')
    axs[0].set_title(f"Raw MRI Slice {image_index}")
    axs[0].axis('off')

    axs[1].imshow(single_sample_targets[image_index].cpu().numpy(), cmap='viridis', vmin=0, vmax=2)
    axs[1].set_title("Ground Truth Segmentation")
    axs[1].axis('off')

    axs[2].imshow(single_sample_preds.cpu().numpy()[image_index], cmap='viridis', vmin=0, vmax=2)
    axs[2].set_title("Model Prediction")
    axs[2].axis('off')

    plt.tight_layout()

    # Save figure as PDF
    save_path = os.path.join(save_dir, f"mri_slice_{image_index}.png")
    plt.savefig(save_path, bbox_inches='tight', dpi=300)

    from google.colab import files
    files.download("mri_segmentation_pdfs/mri_slice_85.png")

    plt.show()

import matplotlib.pyplot as plt

# Initialize model, loss function, & optimizer
dataset = MRIDataset(brats_data_folder, metadata_file, start_slice=2, end_slice=154, grayscale=True)
model = AttentionGatedUNet3D(in_channels=1, out_channels=4, base_channels=18)
model_path = '/content/drive/MyDrive/NTAB_MRI/intermediate_steps/FTL_Loss_alpha0.7_beta0.3_gamma1.3333333333333333_lr0.001_E1_batch_109_loss_0.7500.pth'
# model_path = "/content/drive/MyDrive/NTAB_MRI/FTL_Loss_Attempt_1_1_loss_0.9835003630535023.pth"
# model_path = "/content/drive/MyDrive/NTAB_MRI/model_with_4_channels_1_loss_0.7077021365230148.pth"
model.load_state_dict(torch.load(model_path))
model.cuda() # Put model on the GPU

# dataset[310] is .75 loss (because the target has 3 channels of data)
# dataset[311] is .50 loss (target only has 2 channels of data)
single_sample_inputs, single_sample_targets = dataset[310]
single_sample_inputs = single_sample_inputs.cuda()

single_sample_outputs = model(single_sample_inputs)
single_sample_outputs = single_sample_outputs.permute(1, 2, 3, 0)
single_sample_outputs = torch.softmax(single_sample_outputs, dim=-1)
single_sample_outputs = single_sample_outputs.cpu().detach().numpy()
single_sample_inputs = single_sample_inputs.cpu().numpy()
single_sample_targets = single_sample_targets.cpu().numpy()
for image_index in range(len(single_sample_targets)):
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns
    axs[0].imshow(single_sample_inputs[0][image_index])
    axs[0].axis('off')
    axs[0].set_title("Raw MRI: " + str(image_index))

    target_image = single_sample_targets[image_index][:, :, 1:]
    axs[1].imshow(target_image)
    axs[1].axis('off')
    axs[1].set_title("Target slice: " + str(image_index))

    output_image = single_sample_outputs[image_index][:, :, 1:]
    axs[2].imshow(output_image)
    axs[2].axis('off')
    axs[2].set_title("Output slice: " + str(image_index))

    plt.tight_layout()
    plt.show()

"""# [VISUALIZATION] Animate Outputs"""

import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.animation import PillowWriter
from IPython.display import HTML
import numpy as np
from matplotlib.colors import ListedColormap
import matplotlib.colors as mcolors

def animate_outputs(output, interval=100, save_as_gif=False, gif_filename="slices.gif"):
    """
    Animate all slices of the patient's data smoothly and save as a GIF if required.

    Args:
        patient: The patient data containing slices to display (patient[0][0]).
        interval: Time between frames in milliseconds.
        save_as_gif: Whether to save the animation as a GIF.
        gif_filename: Filename for the saved GIF.
    """
    # inputs = inputs.astype(np.float32) # This is already done outside
    num_outputs = output.shape[0] # Should index second element because it's (152, 240, 240, 3)


    # Create a figure and axis
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.axis('off')  # Turn off axes

    vmin = np.percentile(output[0, :, :, :], 5)  # 5th percentile
    vmax = np.percentile(output[0, :, :, :], 95)  # 95th percentile

    img = ax.imshow(output[0, :, :, :], vmin=vmin, vmax=vmax)  # Adjust vmin/vmax if needed # Correctly index data

    # Function to update the image for each frame
    def update(frame):
        # Update image data with proper indexing
        img.set_data(output[frame, :, :, :]) # outputs is (152, 240, 240, 3)
        return [img]

    # Create the animation
    ani = animation.FuncAnimation(
        fig, update, frames=num_outputs, interval=interval, blit=True
    )

    # Display animation in the notebook
    display(HTML(ani.to_jshtml()))

    # Save as a video if requested
    if save_as_gif:
        writer = PillowWriter(fps=1000 // interval)  # FPS is derived from interval
        ani.save(gif_filename, writer=writer)
        print(f"GIF saved as {gif_filename}")

# Initialize model, loss function, & optimizer
dataset = MRIDataset(brats_data_folder, metadata_file, start_slice=2, end_slice=154, grayscale=True)
model = UNet3D(in_channels=1, out_channels=3, base_channels=32)
model_path = "/content/drive/MyDrive/NTAB_MRI/FR_best_model_old_lr_from_niki_41_epoch_1_loss_0.8136511549756333.pth"
model.load_state_dict(torch.load(model_path))
model.cuda() # Put model on the GPU

# dataset[6] is the overfit from training
# dataset[311] is the lowest loss in validation <-- Evan's personal fav
single_sample_inputs, single_sample_targets = dataset[311]
single_sample_inputs = single_sample_inputs.cuda()

single_sample_outputs = model(single_sample_inputs)
single_sample_outputs = single_sample_outputs.permute(1, 2, 3, 0)
single_sample_outputs = torch.softmax(single_sample_outputs, dim=-1)
single_sample_outputs = single_sample_outputs.cpu().detach().numpy()
single_sample_inputs = single_sample_inputs.cpu().numpy()
single_sample_targets = single_sample_targets.cpu().numpy()

animate_outputs(single_sample_outputs, interval=100, save_as_gif=True, gif_filename="outputs_311_color.gif")

from google.colab import drive
drive.mount('/content/drive')

"""## **Time to Evaluate Your Image!**"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import os
import torch.nn.functional as F
import random

# Load the trained model
model = UNet3D(in_channels=1, out_channels=3, base_channels=32)
# model_path = "/content/drive/MyDrive/NTAB_MRI/FTL_Loss_Attempt_1_1_loss_0.9835003630535023.pth"
# model_path = "/content/drive/MyDrive/NTAB_MRI/FR_best_model_old_lr_from_niki_41_epoch_1_loss_0.8136511549756333.pth"
model.load_state_dict(torch.load(model_path))
model.cuda()
model.eval()  # Set model to evaluation mode

PROCESSED_H5_TENSORS_DIR = "/content/drive/MyDrive/NTAB_MRI/processed_h5/"

# Tumor type mappings
TUMOR_TYPES = ['Necrotic/Tumor Core', 'Edema', 'Enhancing Tumor']

class AdvancedMRIModelEvaluator:
    def __init__(self, model, threshold=0.5):
        self.model = model
        self.threshold = threshold

    def load_preprocessed_image(self, volume_id):
        tensor_path = os.path.join(PROCESSED_H5_TENSORS_DIR, f"grayscale_volume_{volume_id}.pt")
        if os.path.exists(tensor_path):
            volume_data = torch.load(tensor_path)
            # Pad depth to 168 if needed
            target_depth = 168
            if volume_data.shape[1] < target_depth:
                pad_depth = target_depth - volume_data.shape[1]
                pad = (0, 0, 0, 0, pad_depth // 2, pad_depth - pad_depth // 2)
                volume_data = F.pad(volume_data, pad)
            if torch.cuda.is_available():
                volume_data = volume_data.cuda()
            return volume_data
        else:
            raise FileNotFoundError(f"Tensor file for volume {volume_id} not found at {tensor_path}")

    def evaluate_image(self, volume_id):
        image_data = self.load_preprocessed_image(volume_id)
        with torch.no_grad():
            logits = self.model(image_data)
            probabilities = torch.softmax(logits, dim=1).cpu().numpy()  # Shape: (3, depth, height, width)
        return probabilities, image_data.cpu().numpy()

    def classify_images_in_sample(self, volume_ids):
        results = {}
        for volume_id in volume_ids:
            try:
                probabilities, image_data = self.evaluate_image(volume_id)
                results[volume_id] = (probabilities, image_data)
            except FileNotFoundError as e:
                print(f"Error for volume {volume_id}: {e}")
        return results

# Instantiate the evaluator
evaluator = AdvancedMRIModelEvaluator(model)

# Randomly sample 3 volumes
all_volume_ids = list(range(1, 11))  # Adjust range based on dataset
sample_volume_ids = random.sample(all_volume_ids, 3)
print(f"Evaluating volumes: {sample_volume_ids}")

# Evaluate the sampled volumes
results = evaluator.classify_images_in_sample(sample_volume_ids)

# Display results and images with probabilities
for volume_id, (probabilities, image_data) in results.items():
    print(f"Volume: {volume_id}")
    print(f"Displaying normalized probabilities for a random slice:")
    slice_idx = random.randint(0, probabilities.shape[1] - 1)  # Adjust for the depth of probabilities
    print(f"Displaying random slice {slice_idx} for Volume {volume_id}")
    slice_probabilities = probabilities[:, slice_idx, :, :]  # Shape: (3, height, width)
    mean_probs = slice_probabilities.mean(axis=(1, 2))  # Shape: (3,)
    normalized_probs = mean_probs / mean_probs.sum()  # Normalize probabilities
    diagnosis_idx = np.argmax(normalized_probs)  # Tumor type with the highest normalized probability
    diagnosis = TUMOR_TYPES[diagnosis_idx]

    print(f"Diagnosis: {diagnosis}")
    print(f"Normalized Probabilities: {dict(zip(TUMOR_TYPES, normalized_probs))}")

    # Display raw MRI slice
    plt.figure(figsize=(16, 6))
    plt.subplot(1, 4, 1)
    plt.imshow(image_data[0, slice_idx, :, :], cmap='gray')
    plt.title(f"Raw MRI Slice\nDiagnosis: {diagnosis}")
    plt.axis('off')

    # Display normalized probability heatmaps for each class
    for i, tumor_class in enumerate(TUMOR_TYPES):
        plt.subplot(1, 4, i + 2)
        plt.imshow(slice_probabilities[i, :, :], cmap='hot')
        plt.title(f"{tumor_class}\nNormalized Prob: {normalized_probs[i]:.2f}")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

